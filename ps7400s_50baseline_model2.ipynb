{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# Library\n",
    "# ################################################################################\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import glob\n",
    "from os.path import basename\n",
    "import pickle\n",
    "import tqdm\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps7400_통합베이스모델\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ################################################################################\n",
    "# Setting\n",
    "# ################################################################################\n",
    "prefix = 'ps7400'\n",
    "workname = '통합베이스모델'\n",
    "print(prefix + '_' + workname)\n",
    "\n",
    "# output setting\n",
    "output_dir = 'psdata/' + prefix + '/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# Function\n",
    "# ################################################################################\n",
    "def mask_list_load(img_list, mask_dir):\n",
    "    mask_list = []\n",
    "\n",
    "    for img_nm in img_list:\n",
    "        img_filename = basename(img_nm)\n",
    "        mask_filepath = mask_dir + img_filename\n",
    "        mask_list.append(mask_filepath)\n",
    "\n",
    "    return mask_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# Parameter\n",
    "# ################################################################################\n",
    "# 선택사항\n",
    "pc_nm = 'ai'\n",
    "dataset_type = 'dilation' # base - 기본데이터(전체), crack - 크랩만, dilation - 픽셀1개 확장\n",
    "batch_size = 50\n",
    "k_size = 3  # kernel size 1 or 3\n",
    "normalize_yn = 'y' # y or n\n",
    "device_val = 'gpu'\n",
    "acc_interval = 100\n",
    "\n",
    "weight_yn = 'y'\n",
    "weight_val = [0.001, 0.999]\n",
    "\n",
    "para_msg = 'dataset_type::' + dataset_type + '_k_size::' + str(k_size) + '_normal::' + \\\n",
    "    normalize_yn + '_weight::' + weight_yn + '_' + str(weight_val)\n",
    "\n",
    "# 고정사항\n",
    "learning_rate = 1e-5\n",
    "width = height = 512  # image width and height\n",
    "in_channels = 256\n",
    "out_channels = 2\n",
    "\n",
    "# padding size\n",
    "if k_size == 1:\n",
    "    p_size = 0\n",
    "elif k_size == 3:\n",
    "    p_size = 1\n",
    "\n",
    "## 지금 시각을 YYMMDD_HHMMSS 형태로 리턴하기\n",
    "def timetail():\n",
    "    now = datetime.datetime.now()\n",
    "    yy = '{0.year:02}'.format(now)[2:4]\n",
    "    MM = '{0.month:02}'.format(now)\n",
    "    dd = '{0.day:02}'.format(now)\n",
    "    hh = '{0.hour:02}'.format(now)\n",
    "    mm = '{0.minute:02}'.format(now)\n",
    "    ss = '{0.second:02}'.format(now)\n",
    "    timetail = yy + MM + dd + '_' + hh + mm + ss\n",
    "    return timetail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# IO\n",
    "# ################################################################################\n",
    "if dataset_type == 'base':\n",
    "    train_folder = 'ps7120'\n",
    "    mask_folder = 'ps7120'\n",
    "    test_folder = 'ps7120'\n",
    "\n",
    "elif dataset_type == 'crack':\n",
    "    train_folder = 'ps7130'\n",
    "    mask_folder = 'ps7130'\n",
    "    test_folder = 'ps7130'\n",
    "\n",
    "elif dataset_type == 'dilation':\n",
    "    train_folder = 'ps7130'\n",
    "    mask_folder = 'ps7140'\n",
    "    test_folder = 'ps7130'\n",
    "else:\n",
    "    sys.exit('dataset error')\n",
    "\n",
    "train_img_filepath = 'psdata/' + train_folder +'/train/img/*.png'\n",
    "train_mask_filepath = 'psdata/' + mask_folder + '/train/mask/'\n",
    "\n",
    "train_img_list = glob.glob(train_img_filepath)\n",
    "train_mask_list = mask_list_load(train_img_list, train_mask_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# transform\n",
    "# ################################################################################\n",
    "\n",
    "if normalize_yn == 'y':\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        # 전학습된 모델을 사용해서인걸로\n",
    "    ])\n",
    "\n",
    "    test_img_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "            # 전학습된 모델을 사용해서인걸로\n",
    "    ])\n",
    "\n",
    "else:\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    test_img_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# read image and transform to tensor using fransform\n",
    "# ################################################################################\n",
    "def read_images(img_list, mask_list, height, width):\n",
    "    img_cnt = len(img_list)\n",
    "    imgset = torch.zeros(img_cnt, 3, height, width)\n",
    "    maskset = torch.zeros(img_cnt, height, width)\n",
    "\n",
    "    for i in range(len(img_list)):\n",
    "        img = cv2.imread(img_list[i])\n",
    "        mask = cv2.imread(mask_list[i])\n",
    "        imgset[i] = test_img_transform(img)\n",
    "        maskset[i] = mask_transform(mask[:,:,0])\n",
    "    maskset = torch.where(maskset == 0, maskset, torch.tensor(1))\n",
    "\n",
    "    return imgset, maskset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# read random image\n",
    "# ################################################################################\n",
    "def read_random_image():\n",
    "    idx=np.random.randint(0, len(train_img_list))\n",
    "\n",
    "    img = cv2.imread(train_img_list[idx])[:,:,0:3]\n",
    "    mask = cv2.imread(train_mask_list[idx])[:,:,0]\n",
    "    img = img_transform(img)\n",
    "    mask = mask_transform(mask)\n",
    "\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# batch form imgset and maskset\n",
    "# ################################################################################\n",
    "'''\n",
    "def load_batch(batch_size, imgset, maskset):\n",
    "    cnt = len(imgset)\n",
    "    idxs = np.random.choice(cnt, batch_size)\n",
    "    # print(idxs)\n",
    "    return imgset[idxs], maskset[idxs]\n",
    "'''\n",
    "def load_batch2():\n",
    "    imgset_sample = torch.zeros([batch_size,3,height,width])\n",
    "    maskset_sample = torch.zeros([batch_size, height, width])\n",
    "    for i in range(batch_size):\n",
    "        imgset_sample[i], maskset_sample[i] = read_random_image()\n",
    "    return imgset_sample, maskset_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oaiskoo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\oaiskoo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# ################################################################################\n",
    "# Load and set Net and Optimizer\n",
    "# ################################################################################\n",
    "if device_val == 'gpu':\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "#if model_v == deeplabv3_resnet50:\n",
    "Net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "\n",
    "\n",
    "Net.classifier[4] = torch.nn.Conv2d(in_channels,\n",
    "                                    out_channels,\n",
    "                                    kernel_size=k_size,\n",
    "                                    stride=(1, 1),\n",
    "                                    padding=p_size)\n",
    "# out_channels의 의미는 필터임\n",
    "\n",
    "Net = Net.to(device)\n",
    "\n",
    "# Create adma optimizer\n",
    "optimizer = torch.optim.Adam(params=Net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_type::dilation_k_size::3_normal::y_weight::y_[0.001, 0.999]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\oaiskoo\\Desktop\\01_work\\0287_PY_pytorch\\ps7400s_50baseline_model2.ipynb 셀 11\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/oaiskoo/Desktop/01_work/0287_PY_pytorch/ps7400s_50baseline_model2.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# torch.Size([5, 512, 512]) (배치 5기준)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oaiskoo/Desktop/01_work/0287_PY_pytorch/ps7400s_50baseline_model2.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m anns \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mVariable(anns, requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/oaiskoo/Desktop/01_work/0287_PY_pytorch/ps7400s_50baseline_model2.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m Pred \u001b[39m=\u001b[39m Net(imgs)[\u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oaiskoo/Desktop/01_work/0287_PY_pytorch/ps7400s_50baseline_model2.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# torch.Size([5, 2, 512, 512]) 배치 5에 클래스 2\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oaiskoo/Desktop/01_work/0287_PY_pytorch/ps7400s_50baseline_model2.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m Net\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\oaiskoo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\oaiskoo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\segmentation\\_utils.py:23\u001b[0m, in \u001b[0;36m_SimpleSegmentationModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m input_shape \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n\u001b[0;32m     22\u001b[0m \u001b[39m# contract: features is a dict of tensors\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[0;32m     25\u001b[0m result \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m     26\u001b[0m x \u001b[39m=\u001b[39m features[\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\oaiskoo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\oaiskoo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m out \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m name, module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[1;32m---> 69\u001b[0m     x \u001b[39m=\u001b[39m module(x)\n\u001b[0;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers:\n\u001b[0;32m     71\u001b[0m         out_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[1;32mc:\\Users\\oaiskoo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\oaiskoo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\oaiskoo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\oaiskoo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\resnet.py:146\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    144\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[1;32m--> 146\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m    147\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[0;32m    148\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mc:\\Users\\oaiskoo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\oaiskoo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\oaiskoo\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ################################################################################\n",
    "# Train\n",
    "# ################################################################################\n",
    "# 모델 정확도 저장할 df 작성\n",
    "for itr in range(100000):\n",
    "    print(para_msg)\n",
    "    imgs, anns = load_batch2()\n",
    "    imgs = torch.autograd.Variable(imgs, requires_grad=False).to(device)\n",
    "    # torch.Size([5, 512, 512]) (배치 5기준)\n",
    "    anns = torch.autograd.Variable(anns, requires_grad=False).to(device)\n",
    "    Pred = Net(imgs)['out']\n",
    "    # torch.Size([5, 2, 512, 512]) 배치 5에 클래스 2\n",
    "\n",
    "    Net.zero_grad()\n",
    "\n",
    "    if weight_yn == 'y':\n",
    "        weight_loss = torch.FloatTensor(weight_val).to(device)\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=weight_loss)\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    Loss = criterion(Pred, anns.long())\n",
    "    Loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(itr, \") Loss=\", Loss.data.cpu().numpy())\n",
    "\n",
    "    # 모델 정확도 확인 및 저장\n",
    "    if itr % acc_interval == 0:\n",
    "\n",
    "        time_tail = timetail()\n",
    "        if not os.path.exists(output_dir + time_tail):\n",
    "            os.makedirs(output_dir + time_tail)\n",
    "\n",
    "        # 예측 결과\n",
    "        segs = torch.argmax(Pred, 1).cpu().detach()\n",
    "\n",
    "        # 정확도 계산\n",
    "        segs = segs.type(torch.IntTensor)\n",
    "        anns = anns.type(torch.IntTensor)\n",
    "        acc, precision, recall, TPr, TNr, FPr, FNr = pr0287.seg_acc(\n",
    "            segs, anns, batch_size)\n",
    "        loss_val = round(Loss.data.numpy().item(), 5)\n",
    "\n",
    "        miou = pr0287.seg_miou(batch_size, anns, segs, 2)\n",
    "\n",
    "        print('itr: ', itr,\n",
    "              ' Loss: ', Loss.item(),\n",
    "              ' Model Acc: ', acc,\n",
    "              ' Precision: ', precision,\n",
    "              ' Recall: ', recall,\n",
    "              ' TPr: ', TPr,\n",
    "              ' TNr: ', TNr,\n",
    "              ' FPr: ', FPr,\n",
    "              ' FNr: ', FNr,\n",
    "              ' mIoU: ', miou)\n",
    "\n",
    "        # 모델 정확도 저장\n",
    "        model_acc_df = pd.DataFrame(index=range(1),\n",
    "                                    columns=['idx','pc_nm','dataset_type','batch_size',\n",
    "                                                'kernel_size','normalized_yn','weight',\n",
    "                                                'device','itr', 'acc', 'precision',\n",
    "                                                'recall','TPr', 'TNr', 'FPr', 'FNr', 'miou'])\n",
    "\n",
    "        model_acc_df.loc[0, 'pc_nm'] = pc_nm\n",
    "        model_acc_df.loc[0, 'idx'] = time_tail\n",
    "        model_acc_df.loc[0, 'dataset_type'] = dataset_type\n",
    "        model_acc_df.loc[0, 'batch_size'] = batch_size\n",
    "        model_acc_df.loc[0, 'kernel_size'] = k_size\n",
    "        model_acc_df.loc[0, 'normalized_yn'] = normalize_yn\n",
    "        model_acc_df.loc[0, 'device'] = device\n",
    "        model_acc_df.loc[0, 'itr'] = itr\n",
    "        model_acc_df.loc[0, 'acc'] = acc\n",
    "        model_acc_df.loc[0, 'precision'] = precision\n",
    "        model_acc_df.loc[0, 'recall'] = recall\n",
    "        model_acc_df.loc[0, 'TPr'] = TPr\n",
    "        model_acc_df.loc[0, 'TNr'] = TNr\n",
    "        model_acc_df.loc[0, 'FPr'] = FPr\n",
    "        model_acc_df.loc[0, 'FNr'] = FNr\n",
    "        model_acc_df.loc[0, 'miou'] = miou\n",
    "\n",
    "        if weight_yn == 'y':\n",
    "            model_acc_df.loc[0, 'weight'] = str(weight_val)\n",
    "        else:\n",
    "            model_acc_df.loc[0, 'weight'] = '-'\n",
    "\n",
    "        model_acc_df.to_excel(output_dir + time_tail + '/_model_acc.xlsx', index=False)\n",
    "\n",
    "        # 모델 저장\n",
    "        model_filepath = output_dir + time_tail +  '/_crack_model.pth'\n",
    "        torch.save(Net.state_dict(),model_filepath )\n",
    "\n",
    "        # 테스트 파일로 검증\n",
    "        test_img_filepath = 'psdata/' + test_folder + '/test/img/*.png'\n",
    "        test_mask_dir = 'psdata/' + test_folder + '/test/mask/'\n",
    "\n",
    "        test_img_list = glob.glob(test_img_filepath)\n",
    "        test_mask_list = pr0287.mask_list_load(test_img_list, test_mask_dir)\n",
    "\n",
    "        test_imgset, test_maskset = read_images(test_img_list, test_mask_list, height, width)\n",
    "\n",
    "        # load model\n",
    "        test_Net = torchvision.models.segmentation.deeplabv3_resnet50(\n",
    "            pretrained=True)  # Load net\n",
    "        test_Net.classifier[4] = torch.nn.Conv2d(\n",
    "            256, 2, kernel_size=k_size, stride=(1, 1), padding=p_size)\n",
    "        # Change final layer to 2 classes\n",
    "        test_Net = Net.to(device) # Set net to GPU or CPU\n",
    "\n",
    "        test_Net.load_state_dict(torch.load(model_filepath))  # Load trained model\n",
    "        test_Net.eval()  # Set to evaluation mode\n",
    "\n",
    "        # eval\n",
    "        with torch.no_grad():\n",
    "            test_pred = test_Net(test_imgset)['out']  # Run net\n",
    "\n",
    "        test_segs = torch.argmax(test_pred, 1).cpu().detach()\n",
    "        test_segs = test_segs.type(torch.IntTensor)\n",
    "        test_anns = test_maskset.type(torch.IntTensor)\n",
    "\n",
    "        # accuracy test\n",
    "        test_img_cnt = len(test_img_list)\n",
    "        acc, precision, recall, TPr, TNr, FPr, FNr = pr0287.seg_acc(\n",
    "            test_segs, test_anns, test_img_cnt)\n",
    "        miou = pr0287.seg_miou(test_img_cnt, test_anns, test_segs, 2)\n",
    "\n",
    "        print(' model name: ', model_filepath,\n",
    "                ' Model Acc: ', acc,\n",
    "                ' Precision: ', precision,\n",
    "                ' Recall: ', recall,\n",
    "                ' TPr: ', TPr,\n",
    "                ' TNr: ', TNr,\n",
    "                ' FPr: ', FPr,\n",
    "                ' FNr: ', FNr,\n",
    "                ' mIoU: ', miou)\n",
    "\n",
    "        # 결과파일이 없다면\n",
    "        test_acc_df = pd.DataFrame(index=range(1),\n",
    "                            columns=['idx','pc_nm','dataset_type','batch_size',\n",
    "                                        'kernel_size','normalized_yn','weight',\n",
    "                                        'device','itr', 'acc', 'precision',\n",
    "                                        'recall','TPr', 'TNr', 'FPr', 'FNr', 'miou'])\n",
    "        test_acc_df.loc[0, 'pc_nm'] = pc_nm\n",
    "        test_acc_df.loc[0, 'idx'] = time_tail\n",
    "        test_acc_df.loc[0, 'dataset_type'] = dataset_type\n",
    "        test_acc_df.loc[0, 'batch_size'] = batch_size\n",
    "        test_acc_df.loc[0, 'kernel_size'] = k_size\n",
    "        test_acc_df.loc[0, 'normalized_yn'] = normalize_yn\n",
    "        test_acc_df.loc[0, 'device'] = device\n",
    "        test_acc_df.loc[0, 'itr'] = itr\n",
    "        test_acc_df.loc[0, 'acc'] = acc\n",
    "        test_acc_df.loc[0, 'precision'] = precision\n",
    "        test_acc_df.loc[0, 'recall'] = recall\n",
    "        test_acc_df.loc[0, 'TPr'] = TPr\n",
    "        test_acc_df.loc[0, 'TNr'] = TNr\n",
    "        test_acc_df.loc[0, 'FPr'] = FPr\n",
    "        test_acc_df.loc[0, 'FNr'] = FNr\n",
    "        test_acc_df.loc[0, 'miou'] = miou\n",
    "\n",
    "        if weight_yn == 'y':\n",
    "            test_acc_df.loc[0, 'weight'] = str(weight_val)\n",
    "        else:\n",
    "            test_acc_df.loc[0, 'weight'] = '-'\n",
    "\n",
    "        test_acc_df.to_excel(output_dir + time_tail +  '/_test_acc.xlsx', index=False)\n",
    "\n",
    "\n",
    "        # 개별 이미지 결과 저장\n",
    "        for i in range(test_img_cnt):\n",
    "            test_img = cv2.imread(test_img_list[i])\n",
    "            test_seg = test_segs[i].numpy()\n",
    "            test_ann = test_anns[i].numpy()\n",
    "            test_ann = np.where(test_ann == 0, 255, 0)\n",
    "\n",
    "            imgR = test_img[:, :, 0]\n",
    "            imgG = test_img[:, :, 1]\n",
    "            imgB = test_img[:, :, 2]\n",
    "\n",
    "            imgR_s = np.where(test_seg == 1, 0, imgR)\n",
    "            imgG_s = np.where(test_seg == 1, 0, imgG)\n",
    "            imgB_s = np.where(test_seg == 1, 0, imgB)\n",
    "\n",
    "            imgR_m = np.concatenate((imgR, test_ann, imgR_s), axis=1)\n",
    "            imgG_m = np.concatenate((imgG, test_ann, imgG_s), axis=1)\n",
    "            imgB_m = np.concatenate((imgB, test_ann, imgB_s), axis=1)\n",
    "\n",
    "            img_m = np.dstack((imgR_m, imgG_m, imgB_m))\n",
    "\n",
    "            cv2.imwrite(output_dir + time_tail + '/seg_' + basename(test_img_list[i]), img_m)\n",
    "\n",
    "\n",
    "        # output_dir 아래에 있는 모든 폴더 리스트 가져오기\n",
    "        try:\n",
    "            rlt_dirs = glob.glob(output_dir + '*/')\n",
    "            all_model_acc_df = pd.DataFrame()\n",
    "            all_test_acc_df = pd.DataFrame()\n",
    "\n",
    "            # 각 폴더에 포함된 _model_acc_xlsx 파일 리스트 가져오기\n",
    "            for rlt_dir in rlt_dirs:\n",
    "\n",
    "                # 폴더 안에 _model_acc_xlsx 파일이 있으면\n",
    "                if len(glob.glob(rlt_dir + '_model_acc.xlsx')) > 0:\n",
    "                    # 해당 파일을 불러와서\n",
    "                    rlt_df = pd.read_excel(rlt_dir + '_model_acc.xlsx')\n",
    "                    # 해당 파일에 새로운 결과를 추가\n",
    "                    all_model_acc_df = pd.concat([all_model_acc_df, rlt_df], axis=0)\n",
    "                    # 결과를 저장\n",
    "            all_model_acc_df.to_excel(output_dir + 'all_model_acc.xlsx', index=False)\n",
    "\n",
    "            # 각 폴더에 포함된 _test_acc_xlsx 파일 리스트 가져오기\n",
    "            for rlt_dir in rlt_dirs:\n",
    "\n",
    "                # 폴더 안에 _model_acc_xlsx 파일이 있으면\n",
    "                if len(glob.glob(rlt_dir + '_test_acc.xlsx')) > 0:\n",
    "                    # 해당 파일을 불러와서\n",
    "                    rlt_df = pd.read_excel(rlt_dir + '_test_acc.xlsx')\n",
    "                    # 해당 파일에 새로운 결과를 추가\n",
    "                    all_test_acc_df = pd.concat([all_test_acc_df, rlt_df], axis=0)\n",
    "                    # 결과를 저장\n",
    "            all_test_acc_df.to_excel(output_dir + 'all_test_acc.xlsx', index=False)\n",
    "\n",
    "        except:\n",
    "            print('collect result is failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# Finish\n",
    "# ################################################################################\n",
    "print(\"time :\", time.time() - start)\n",
    "print('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "992f425e58c3379c74963fffd7aafb953d75ec811aaddd2ee8a010590892053a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
